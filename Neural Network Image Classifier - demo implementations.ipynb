{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Demo - Image Classifier\n",
    "\n",
    "A python implementation of [Andrew Ng's neural network algorithm](https://www.coursera.org/learn/machine-learning/home/week/5) - classic, from-scratch implementation.\n",
    "\n",
    "Resources:\n",
    "1. [Katherine Bailey's post, 'Neural Nets in Python'](https://katbailey.github.io/post/neural-nets-in-python/)\n",
    "2. [Keon Yong Lee's post, 'Deep Learning by Andrew Ng â€” Course 1: Neural Networks and Deep Learning'](https://medium.com/@keonyonglee/bread-and-butter-from-deep-learning-by-andrew-ng-course-1-neural-networks-and-deep-learning-41563b8fc5d8)\n",
    "3. [Benjamin Lau's post, 'Andrew Ng's Machine Learning Course in Python'](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9)\n",
    "\n",
    "Later, may also implement a neural network for image classification using [TensorFlow](https://www.tensorflow.org/tutorials/images/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import scipy.io as sio #use to import matlab dataset\n",
    "from sklearn.model_selection import train_test_split #note, this differs from Katherine Bailey's code: sklearn.cross_validation\n",
    "import matplotlib.pyplot as plt #note, using pyplot instead of pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset - 5000 handwritten digits between 0 and 9\n",
    "mat_contents = sio.loadmat('data-ex4/ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  (5000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [9],\n",
       "       [9],\n",
       "       [9]], dtype=uint8)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract y labels:\n",
    "# 0s were converted to 10s in the matlab data because matlab\n",
    "# indices start at 1, so we need to change them back to 0s\n",
    "y = mat_contents['y'] #a numpy ndarray\n",
    "y = np.where(y == 10, 0, y)\n",
    "print('size: ',y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9], dtype=uint8)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the transform of the y labels - converts from column to row vector\n",
    "y = y.reshape((y.shape[0],)) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mat_contents['X']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. training examples: 3500\n",
      "No. test examples: 1500\n"
     ]
    }
   ],
   "source": [
    "#split data: 70% data for training, 30% for testing:\n",
    "X_train, X_test, y_train, y_test = train_test_split(mat_contents['X'], y,test_size=0.3,random_state=10) \n",
    "print('No. training examples:',y_train.shape[0])\n",
    "print('No. test examples:', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a cross-validation set? come back to this\n",
    "#X_train, X_val = X_train[:-1000], X_train[-1000:]\n",
    "#y_train, y_val = y_train[:-1000], y_train[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93c4a57850>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#inspect one of the X examples\n",
    "plt.imshow(X_train[1301].reshape((20, 20), order='F'), cmap='Greys',  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use minimize function to optimize theta parameters - NEED THIS?\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    '''Basic sigmoid function for logistic regression.'''\n",
    "    return 1.0 / (1.0 + e ** (-1.0 * X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(X):\n",
    "    '''Adds the bias column to the matrix X'''\n",
    "    return np.concatenate((np.ones((X.shape[0],1)), X), 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    '''Randomly initializes the weights for layer with the specified numbers of input and output nodes.'''\n",
    "    #Randomly initialize the weights to small values\n",
    "    epsilon_init = 0.1\n",
    "    #generate random normal distribution with a mean of 0.5\n",
    "    W = np.random.normal(0.5,size=(L_out, 1 + L_in)) * (2 * epsilon_init) - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInitialWeights(input_layer_size, hidden_layer_size, num_labels):\n",
    "    '''Returns a single vector of randomly initialized theta1, theta2 parameters (weights) for the\n",
    "    input layer and hidden layer.'''\n",
    "    theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "    theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "    return np.append(theta1.ravel(order='A'), theta2.ravel(order='A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeThetaMatrices(thetas, input_layer_size, hidden_layer_size, num_labels):\n",
    "    '''Reshapes the two theta matrices from a single vector, given the\n",
    "    size of the input layer, the hidden layer, and the number of\n",
    "    labels in the output.'''\n",
    "    theta1size = (input_layer_size + 1) * hidden_layer_size\n",
    "    theta1 = np.reshape(thetas[:theta1size], (hidden_layer_size, input_layer_size + 1), order='A')\n",
    "    theta2 = np.reshape(thetas[theta1size:], (num_labels, hidden_layer_size + 1), order='A')\n",
    "    return theta1, theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y, num_classes):\n",
    "    '''Converts y vector of labels via one-hot encoding to matrix of classes'''\n",
    "    y = y.reshape((y.shape[0],1))\n",
    "    ycols = np.tile(y, (1, num_classes))\n",
    "    m, n = ycols.shape\n",
    "    indices = np.tile(np.arange(num_classes).reshape((1,num_classes)), (m, 1))\n",
    "    ymat = indices == ycols\n",
    "    return ymat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test oneHotEncode function\n",
    "oneHotEncode(y,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network with forward propagation, prediction, cost calculation, back propagation, update of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, num_labels, hidden_layer_size, lmda, maxIter):\n",
    "    '''Trains a basic neural network, assuming 3 layers: input, hidden, output. \n",
    "    Returns parameters (weights) to use to predict new data.'''\n",
    "    input_layer_size = X_train.shape[1]\n",
    "    initial_weights = createInitialWeights(input_layer_size, hidden_layer_size, num_labels)\n",
    "    \n",
    "    if y_train.ndim == 1:\n",
    "        # Convert the y labels to one-hot vectors.\n",
    "        y_train = oneHotEncode(y_train, num_labels)\n",
    "\n",
    "    ## reshapeThetaMatrices? \n",
    "    \n",
    "    def getActivations(theta1, theta2):\n",
    "        '''Given theta parameters for the input layer and hidden layer, calulate the \n",
    "        activation values for the hidden layer (a2) and the output layer (a3).'''\n",
    "        a1 = addBias(X_train)\n",
    "        z2 = np.dot(a1,theta1.T)\n",
    "        a2 = np.concatenate((np.ones((z2.shape[0],1)), sigmoid(z2)), 1)\n",
    "        # a2 is an m x num_hidden+1 matrix, Theta2 is a num_labels x\n",
    "        # num_hidden+1 matrix\n",
    "        z3 = np.dot(a2,theta2.T)\n",
    "        a3 = sigmoid(z3) # Now we have an m x num_labels matrix\n",
    "        return a2, a3\n",
    "    \n",
    "    def costFunction(thetas):\n",
    "        '''Cost function to be minimized with respect to weights.'''\n",
    "        m = X_train.shape[0]\n",
    "        \n",
    "        theta1, theta2 = reshapeThetaMatrices(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "        a2, a3 = getActivations(theta1, theta2)\n",
    "        \n",
    "        h = a3 #hypothesis\n",
    "        \n",
    "        #calculate cost without regularization\n",
    "        cost = (1/m) * np.sum((-y_train * np.log(h)) - ((1 - y_train) * np.log(1-h))) #note np.sum is required here rather than sum\n",
    "        \n",
    "        \n",
    "        # Regularization\n",
    "        #NOTE, not including the bias first columns for each theta matrix\n",
    "        thetasq = np.sum(theta1[:,1:(input_layer_size + 1)]**2) + np.sum(theta2[:,1:hidden_layer_size + 1]**2)\n",
    "        reg = (lmda /(2.*m)) * thetasq\n",
    "        reg_cost = cost + reg\n",
    "        print(\"Training loss:\\t\\t{:.6f}\".format(cost))\n",
    "        return reg_cost\n",
    "    \n",
    "    \n",
    "    def calculateGradient(thetas):\n",
    "        '''Gradient function to pass to our optimization function.'''\n",
    "        m = X_train.shape[0]\n",
    "        theta1, theta2 = extractWeightMatrices(weights, input_layer_size, hidden_layer_size, num_labels)\n",
    "        # Backpropagation - step 1: feed-forward.\n",
    "        a2, a3 = getActivations(theta1, theta2)\n",
    "        \n",
    "        # calc delta_3 - the delta error in the output layer is just the difference\n",
    "        # between the output layer, a3, and y\n",
    "        delta_3 = a3 - y_train # delta_3 is m x num_labels\n",
    "        delta_3 = delta_3.T\n",
    "\n",
    "        # calc delta_2\n",
    "        sigmoidGrad = a2 * (1 - a2)\n",
    "        delta_2 = (np.dot(theta2.T,delta_3)) * sigmoidGrad.T\n",
    "        delta_2 = delta_2[1:, :] # hidden_layer_size x m. note first column is excluded because the hidden layer bias unit has no connection to the input layer - so we do not use backpropagation for it.\n",
    "        \n",
    "        #Accumulate errors into Delta matrices\n",
    "        theta1_grad = np.dot(delta_2, np.concatenate((np.ones((X_train.shape[0],1)), X_train), 1)) #delta2 * a1(X with a column of ones)\n",
    "        theta2_grad = np.dot(delta_3, a2) #delta3 * a2\n",
    "        \n",
    "        # Add regularization\n",
    "        reg_grad1 = (lmda / float(m)) * theta1\n",
    "        # We don't regularize the weight for the bias column\n",
    "        reg_grad1[:,0] = 0\n",
    "        \n",
    "        reg_grad2 = (lmda / float(m)) * theta2;\n",
    "        reg_grad2[:,0] = 0\n",
    "        \n",
    "        theta1_grad = ravel((theta1_grad / float(m)) + reg_grad1, order='A') #np.ravel 'flat-packs an array into 1D'\n",
    "        theta2_grad = ravel((theta2_grad / float(m)) + reg_grad2, order='A')\n",
    "        #return a 1D combined array of both gradients\n",
    "        return np.append(theta1_grad,theta2_grad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # START HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 25\n"
     ]
    }
   ],
   "source": [
    "# code below is testing the individual functions above\n",
    "input_layer_size = X_train.shape[1]\n",
    "hidden_layer_size=25\n",
    "lmda = 2\n",
    "print(input_layer_size,hidden_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = createInitialWeights(400, 25, 10)\n",
    "theta1,theta2=reshapeThetaMatrices(thetas, 400, 25, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = addBias(X_train)\n",
    "z2 = np.dot(a1,theta1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.concatenate((np.ones((z2.shape[0],1)), sigmoid(z2)), 1)\n",
    "z3 = np.dot(a2,theta2.T)\n",
    "a3 = sigmoid(z3) # Now we have an m x num_labels matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if y_train.ndim == 1:\n",
    "        # Convert the y labels to one-hot vectors.\n",
    "        y_train = oneHotEncode(y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "h=a3\n",
    "J = (1/m) * np.sum((-y_train * np.log(h)) - ((1 - y_train) * np.log(1-h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2725784005396195"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414.82801625185857"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetasq = np.sum(theta1[:,1:(input_layer_size + 1)]**2) + np.sum(theta2[:,1:hidden_layer_size + 1]**2)\n",
    "thetasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:\t\t6.391101\n"
     ]
    }
   ],
   "source": [
    "lmda = 2\n",
    "reg = (lmda /(2*m)) * thetasq\n",
    "cost = J + reg\n",
    "print(\"Training loss:\\t\\t{:.6f}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # calc delta_3 - the delta error in the output layer is just the difference\n",
    "        # between the output layer, a3, and y\n",
    "        delta_3 = a3 - y_train # delta_3 is m x num_labels\n",
    "        delta_3 = delta_3.T\n",
    "\n",
    "        # calc delta_2\n",
    "        sigmoidGrad = a2 * (1 - a2)\n",
    "        delta_2 = (np.dot(theta2.T,delta_3)) * sigmoidGrad.T\n",
    "        delta_2 = delta_2[1:, :] # hidden_layer_size x m. note first column is excluded because the hidden layer bias unit has no connection to the input layer - so we do not use backpropagation for it.\n",
    "        \n",
    "        #Accumulate errors into Delta matrices\n",
    "        theta1_grad = np.dot(delta_2, np.concatenate((np.ones((X_train.shape[0],1)), X_train), 1)) #delta2 * a1(X with a column of ones)\n",
    "        theta2_grad = np.dot(delta_3, a2) #delta3 * a2\n",
    "        \n",
    "        # Add regularization\n",
    "        reg_grad1 = (lmda / float(m)) * theta1\n",
    "        # We don't regularize the weight for the bias column\n",
    "        reg_grad1[:,0] = 0\n",
    "        \n",
    "        reg_grad2 = (lmda / float(m)) * theta2;\n",
    "        reg_grad2[:,0] = 0\n",
    "        \n",
    "        theta1_grad = np.ravel((theta1_grad / float(m)) + reg_grad1, order='A') \n",
    "        theta2_grad = np.ravel((theta2_grad / float(m)) + reg_grad2, order='A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.93763090e-03, -1.44216197e-04, -2.33366989e-05, ...,\n",
       "       -2.10759087e-04,  9.25626012e-05, -1.86607735e-04])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.93763090e-03, -1.44216197e-04, -2.33366989e-05, ...,\n",
       "        9.81530413e-02,  1.49853804e-01,  1.41802847e-01])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(theta1_grad,theta2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
